import requests
from lxml import html
import pandas as pd
import random
from itertools import chain
import os
from fake_useragent import UserAgent
from tqdm import tqdm
import pickle
import time
import parmap
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import asyncio
from playwright.async_api import async_playwright
import math
import json

# ì„¸ì…˜ ì„¤ì • (ì¬ì‚¬ìš© ë° ì¬ì‹œë„ ì„¤ì •)
def create_session():
    session = requests.Session()
    
    # ì¬ì‹œë„ ì„¤ì •
    retry_strategy = Retry(
        total=3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"],  # method_whitelist -> allowed_methods
        backoff_factor=1
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

# í—¤ë” ìƒì„±
def get_headers():
    ua = UserAgent(platforms='desktop')
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

# Playwright ê´€ë ¨ í•¨ìˆ˜ë“¤
async def create_page(playwright, user_agent, headless=True):
    """Playwright í˜ì´ì§€ ìƒì„±"""
    browser = None
    try:
        browser = await playwright.chromium.launch(
            headless=headless,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-extensions',
                '--disable-default-apps',
                '--disable-gpu'
            ]
        )
        context = await browser.new_context(
            user_agent=user_agent,
            is_mobile=False,
            has_touch=False,
            viewport={'width': 1920, 'height': 1080}
        )
        page = await context.new_page()
        return page, browser
    except Exception as e:
        print(f'Playwright í˜ì´ì§€ ìƒì„± ì—ëŸ¬: {e}')
        return None, None

async def login_playwright(page):
    """Playwrightë¡œ ë¡œê·¸ì¸ ì²˜ë¦¬"""
    await page.goto('https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all')
    
    # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    login_button = page.locator('xpath=//*[@id="gnb_login_button"]')
    if await login_button.count() > 0:
        await login_button.click()

    with open('data/naver_id_pw.json', 'r', encoding='utf-8') as f:
        data = json.load(f)

    id = data['id']
    pw = data['pw']
    await page.fill('xpath=//*[@id="id"]', id)
    await page.fill('xpath=//*[@id="pw"]', pw)

    await page.locator('xpath=//*[@id="log.login"]').click()
    print("ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
    input()  # ì‚¬ìš©ìê°€ ìˆ˜ë™ìœ¼ë¡œ ë¡œê·¸ì¸ ì™„ë£Œí•  ë•Œê¹Œì§€ ëŒ€ê¸°

async def extract_xpath_playwright(page, xpaths, attr_type='text'):
    """Playwrightì—ì„œ XPath ì¶”ì¶œ"""
    for xpath in xpaths:
        try:
            element = page.locator(f'xpath={xpath}')
            if await element.count() > 0:
                if attr_type == 'text':
                    data = await element.first.inner_text()
                elif attr_type == 'src':
                    data = await element.first.get_attribute('src')
                elif attr_type == 'href':
                    data = await element.first.get_attribute('href')
                
                if data and data.strip():
                    if '_ë‹˜ë¡œê·¸ì•„ì›ƒ' in data:
                        continue
                    return data.strip()
        except Exception as e:
            continue
    return ''

async def get_data_playwright(page, url, age=19):
    """Playwrightë¡œ 19ê¸ˆ ì†Œì„¤ ë°ì´í„° ì¶”ì¶œ"""
    try:
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            response = await page.goto(url, timeout=30000)
            
            if response.status != 200:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"HTTP {response.status} ì˜¤ë¥˜. ì¬ì‹œë„ {retry_count}/{max_retries}")
                    await asyncio.sleep(random.uniform(1, 3))
                    continue
                else:
                    raise Exception(f'HTTP {response.status} ì˜¤ë¥˜ê°€ {max_retries}ë²ˆ ë°˜ë³µë¨')
            break

        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await asyncio.sleep(1)

        # ë°ì´í„° ì¶”ì¶œ (Playwright ë°©ì‹)
        img_xpaths = [
            '/html/body/div[1]/div[2]/div[1]/span/img',
            '//*[@id="container"]/div[1]/a/img',
            '//*[@id="container"]/div[1]/span/img',
            '//*[@id="ct"]/div[1]/div[1]/div[1]/div[1]/a/img'
        ]
        img = await extract_xpath_playwright(page, img_xpaths, 'src')

        title_xpaths = [
            '//*[@id="content"]/div[1]/h2',
            '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/strong',
            '//*[@id="content"]/div[2]',
            '//*[@id="content"]/div[2]/h2'
        ]
        title = await extract_xpath_playwright(page, title_xpaths)

        rating_xpaths = [
            '//*[@id="content"]/div[1]/div[1]/em',
            '//*[@id="content"]/div[2]/div[1]',
            '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[1]/ul/li/span/span'
        ]
        rating = await extract_xpath_playwright(page, rating_xpaths)

        genre_xpaths = [
            '//*[@id="content"]/ul[1]/li/ul/li[2]/span/a',
            '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[2]'
        ]
        genre = await extract_xpath_playwright(page, genre_xpaths)

        serial_xpaths = [
            '//*[@id="content"]/ul[1]/li/ul/li[1]/span',
            '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[1]'
        ]
        serial = await extract_xpath_playwright(page, serial_xpaths)

        publisher_xpaths = ['//*[@id="content"]/ul[1]/li/ul/li[3]']
        author_xpaths = [
            '//*[@id="content"]/ul[1]/li/ul/li[4]/a',
            '//*[@id="content"]/ul[1]/li/ul/li[3]/a'
        ]
        publisher = await extract_xpath_playwright(page, publisher_xpaths)
        author = await extract_xpath_playwright(page, author_xpaths)

        # ì„¤ëª… - "ë”ë³´ê¸°" ë²„íŠ¼ í´ë¦­ ì²˜ë¦¬
        more_button = page.locator('xpath=//*[@id="content"]/div[2]/div[1]/span/a')
        if await more_button.count() > 0:
            await more_button.click()
            await asyncio.sleep(0.5)

        summary_xpaths = [
            '//*[@id="content"]/div[2]/div[2]',
            '//*[@id="content"]/div[2]/div'
        ]
        summary = await extract_xpath_playwright(page, summary_xpaths)

        page_count_xpaths = ['//*[@id="content"]/h5/strong']
        page_count = await extract_xpath_playwright(page, page_count_xpaths)

        page_unit_xpaths = ['//*[contains(@class, "end_total_episode")]']
        page_unit = await extract_xpath_playwright(page, page_unit_xpaths)
        
        try:
            if page_unit:
                page_unit = page_unit.strip()[-1]
        except:
            print(f"í˜ì´ì§€ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨: {page_unit}")

        novel_data = {
            'url': url,
            'img': img,
            'title': title,
            'author': author,
            'rating': rating,
            'genre': genre,
            'serial': serial,
            'publisher': publisher,
            'summary': summary,
            'page_count': page_count,
            'page_unit': page_unit,
            'age': 19 if age == 19 else 'ì „ì²´',
            'platform': 'naver'
        }

        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit]):
            print(f"ë°ì´í„° ëˆ„ë½: {url}")
            print(novel_data)
            raise Exception('ë°ì´í„° ëˆ„ë½ ë°œìƒ')

        await asyncio.sleep(random.uniform(1, 2))
        return novel_data

    except Exception as e:
        print(f"Playwright ë°ì´í„° ì¶”ì¶œ ì—ëŸ¬ {url}: {e}")
        return None
    


# async def get_19(nineteen_links):
#     """19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ (Playwright ì‚¬ìš©)"""
#     ua = UserAgent(platforms='desktop')
#     all_results = []

#     async with async_playwright() as playwright:
#         page, browser = await create_page(playwright, ua.random, headless=False)
#         if not page:
#             print("Playwright í˜ì´ì§€ ìƒì„± ì‹¤íŒ¨")
#             return []
            
#         await login_playwright(page)
#         await asyncio.sleep(1)
        
#         with tqdm(nineteen_links, desc=f"í˜„ì¬ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(all_results)}", total=len(nineteen_links)) as pbar:
#             for url in pbar:
#                 novel_data = await get_data_playwright(page, url, age=19)
#                 if novel_data:
#                     all_results.append(novel_data)

#                 # tqdm ì„¤ëª… ë™ì  ì—…ë°ì´íŠ¸
#                 pbar.set_description(f"í˜„ì¬ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(all_results)}")
#                 await asyncio.sleep(random.uniform(1, 2))
                
#         if browser:
#             await browser.close()
    
#     return all_results

async def get_19(nineteen_links):
    """19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ (Playwright ì‚¬ìš©)"""
    ua = UserAgent(platforms='desktop')
    browsers = []
    pages = []
    all_results = []
    
    async with async_playwright() as playwright:
        # 1ë‹¨ê³„: 5ê°œ ë¸Œë¼ìš°ì € ìƒì„± ë° ë¡œê·¸ì¸
        print("5ê°œ ë¸Œë¼ìš°ì € ìƒì„± ë° ë¡œê·¸ì¸ ì¤‘...")
        for i in range(5):
            print(f"ë¸Œë¼ìš°ì € {i+1} ìƒì„± ì¤‘...")
            
            # ë¸Œë¼ìš°ì € ìƒì„±
            browser = await playwright.chromium.launch(headless=False)
            
            # í˜ì´ì§€ ìƒì„±
            page = await browser.new_page()
            await page.set_extra_http_headers({
                'User-Agent': ua.random
            })
            
            # ë¡œê·¸ì¸
            await login_playwright(page)
            print(f"ë¸Œë¼ìš°ì € {i+1} ë¡œê·¸ì¸ ì™„ë£Œ")
            
            browsers.append(browser)
            pages.append(page)
            
            await asyncio.sleep(1)
        
        print("ëª¨ë“  ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ! í¬ë¡¤ë§ ì‹œì‘...")
        
        # 2ë‹¨ê³„: URLì„ 5ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        def split_urls(urls, num_parts):
            chunk_size = math.ceil(len(urls) / num_parts)
            chunks = []
            for i in range(num_parts):
                start = i * chunk_size
                end = min((i + 1) * chunk_size, len(urls))
                chunks.append(urls[start:end])
            return chunks
        
        url_chunks = split_urls(nineteen_links, 5)
        
        # 3ë‹¨ê³„: ê° ë¸Œë¼ìš°ì €ê°€ ì²˜ë¦¬í•  í•¨ìˆ˜
        async def process_chunk(page, urls, browser_id, pbar):
            results = []
            for url in urls:
                try:
                    novel_data = await get_data_playwright(page, url, age=19)
                    if novel_data:
                        results.append(novel_data)
                    
                    # tqdm ì—…ë°ì´íŠ¸
                    pbar.update(1)
                    pbar.set_description(f"í˜„ì¬ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(all_results) + sum(len(r) for r in [results])}")
                    
                    await asyncio.sleep(random.uniform(1, 2))
                except Exception as e:
                    print(f"\në¸Œë¼ìš°ì € {browser_id} ì˜¤ë¥˜: {e}")
                    pbar.update(1)
            return results
        
        # 4ë‹¨ê³„: 5ê°œ ë¸Œë¼ìš°ì €ë¡œ ë³‘ë ¬ ì‹¤í–‰ (tqdm ì ìš©)
        tasks = []
        
        # tqdm ì´ˆê¸°í™”
        with tqdm(total=len(nineteen_links), desc="í˜„ì¬ ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: 0") as pbar:
            for i in range(5):
                if i < len(url_chunks) and url_chunks[i]:  # ë¹ˆ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°
                    task = process_chunk(pages[i], url_chunks[i], i+1, pbar)
                    tasks.append(task)
            
            # ëª¨ë“  ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
            results_lists = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ í•©ì¹˜ê¸°
        for results in results_lists:
            all_results.extend(results)
        
        print(f"í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(all_results)}ê°œ ìˆ˜ì§‘")
        
        # 5ë‹¨ê³„: ë¸Œë¼ìš°ì € ë‹«ê¸°
        for i, browser in enumerate(browsers):
            await browser.close()
            print(f"ë¸Œë¼ìš°ì € {i+1} ë‹«ìŒ")
    
    return all_results


def get_login_cookies_selenium():
    """Seleniumì„ ì‚¬ìš©í•´ ë¡œê·¸ì¸ í›„ ì¿ í‚¤ ë°˜í™˜"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        options = Options()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        
        driver = webdriver.Chrome(options=options)
        
        try:
            # ë¡œê·¸ì¸ í˜ì´ì§€ ì´ë™
            driver.get('https://nid.naver.com/nidlogin.login')
            
            # ìˆ˜ë™ ë¡œê·¸ì¸ ëŒ€ê¸°
            print("ë¸Œë¼ìš°ì €ì—ì„œ ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
            input()
            
            # ë¡œê·¸ì¸ í™•ì¸
            if 'series.naver.com' not in driver.current_url:
                driver.get('https://series.naver.com')
            
            # ì¿ í‚¤ ì¶”ì¶œ
            cookies = {}
            for cookie in driver.get_cookies():
                cookies[cookie['name']] = cookie['value']
            
            print(f"ì¶”ì¶œëœ ì¿ í‚¤ ìˆ˜: {len(cookies)}ê°œ")
            return cookies
            
        finally:
            driver.quit()
            
    except ImportError:
        raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install selenium")
    except Exception as e:
        raise Exception(f"Selenium ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")

# ë°ì´í„° ë‚˜ëˆ„ê¸°
def split_data(data, split_num):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ë¶„í•  í•¨ìˆ˜"""
    if isinstance(data, list):
        new_data = []
        for i in range(0, len(data), split_num):
            new_data.append(data[i: i+split_num])
        return new_data
    elif isinstance(data, dict):
        items = list(data.items())
        new_data = []
        for i in range(0, len(items), split_num):
            batch_items = items[i: i+split_num]
            batch_dict = dict(batch_items)
            new_data.append(batch_dict)
        return new_data
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì…ë‹ˆë‹¤: {type(data)}")

# ë§í¬ ê°€ì ¸ì˜¤ê¸°
def get_links(url):
    session = create_session()
    headers = get_headers()
    
    try:
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        tree = html.fromstring(response.content)
        links = []
        
        # h3 íƒœê·¸ë“¤ ì°¾ê¸°
        h3_elements = tree.xpath('//*[@id="content"]/div/ul/li/div/h3')
        
        for h3_element in h3_elements:
            # ì œëª© ê°€ì ¸ì˜¤ê¸°
            page_title = h3_element.text_content().strip()
            
            # a íƒœê·¸ ì°¾ê¸° (h3 ë‚´ë¶€ì—ì„œ)
            a_elements = h3_element.xpath('.//a[contains(@href, "/novel/detail")]')
            
            if a_elements:
                href = a_elements[0].get('href')
                link = 'https://series.naver.com' + href
                
                # ë‚˜ì´ ì œí•œ ì²´í¬
                age = 19 if '19ê¸ˆ' in page_title else 0
                
                links.append({
                    'url': link,
                    'age': age,
                })
        
        time.sleep(random.randint(1, 2))
        return links
        
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì—ëŸ¬ {url}: {e}")
        return []
    finally:
        session.close()

def get_last_page():
    session = create_session()
    headers = get_headers()
    
    try:
        url = 'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&page=100000'
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        tree = html.fromstring(response.content)
        page_elements = tree.xpath('//*[@id="content"]/p/a/text()')
        
        if page_elements:
            max_page_num = max(page_elements)
            return max_page_num
        return "1"
        
    except Exception as e:
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return "1"
    finally:
        session.close()

# XPath ì¶”ì¶œ í•¨ìˆ˜
def extract_xpath(tree, xpaths, attr=None):
    for xpath in xpaths:
        try:
            elements = tree.xpath(xpath)
            if elements:
                if attr:
                    data = elements[0].get(attr)
                else:
                    data = elements[0].text_content()
                
                if data and data.strip():
                    if '_ë‹˜ë¡œê·¸ì•„ì›ƒ' in data:
                        continue
                    return data.strip()
        except Exception as e:
            continue
    return ''

# ì´ë¯¸ì§€ ì¶”ì¶œ
def get_img(tree):
    img_xpaths = [
        '/html/body/div[1]/div[2]/div[1]/span/img',
        '//*[@id="container"]/div[1]/a/img',
        '//*[@id="container"]/div[1]/span/img',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[1]/a/img'
    ]
    return extract_xpath(tree, img_xpaths, attr='src')

# ì œëª© ì¶”ì¶œ
def get_title(tree):
    title_xpaths = [
        '//*[@id="content"]/div[1]/h2',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/strong',
        '//*[@id="content"]/div[2]',
        '//*[@id="content"]/div[2]/h2'
    ]
    return extract_xpath(tree, title_xpaths)

# í‰ì  ì¶”ì¶œ
def get_rating(tree):
    rating_xpaths = [
        '//*[@id="content"]/div[1]/div[1]/em',
        '//*[@id="content"]/div[2]/div[1]',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[1]/ul/li/span/span',
    ]
    return extract_xpath(tree, rating_xpaths)

# ì¥ë¥´ ì¶”ì¶œ
def get_genre(tree):
    genre_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[2]/span/a',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[2]'
    ]
    return extract_xpath(tree, genre_xpaths)

# ì—°ì¬ ìƒíƒœ ì¶”ì¶œ
def get_serial(tree):
    serial_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[1]/span',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[1]'
    ]
    return extract_xpath(tree, serial_xpaths)

# ì¶œíŒì‚¬ ë° ì‘ê°€ ì¶”ì¶œ
def get_publisher_author(tree):
    publisher_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[3]'
    ]
    author_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[4]/a',
        '//*[@id="content"]/ul[1]/li/ul/li[3]/a'
    ]
    publisher = extract_xpath(tree, publisher_xpaths)
    author = extract_xpath(tree, author_xpaths)
    return publisher, author

# ì„¤ëª… ì¶”ì¶œ
def get_summary(tree):
    summary_xpaths = [
        '//*[@id="content"]/div[2]/div[2]',
        '//*[@id="content"]/div[2]/div'
    ]
    return extract_xpath(tree, summary_xpaths)

# í™”ìˆ˜ ì¶”ì¶œ
def get_page_count(tree):
    page_count_xpaths = [
        '//*[@id="content"]/h5/strong'
    ]
    return extract_xpath(tree, page_count_xpaths)

# ë‹¨ìœ„ ì¶”ì¶œ
def get_page_unit(tree):
    page_unit_xpaths = [
        '//*[contains(@class, "end_total_episode")]'
    ]
    page_unit = extract_xpath(tree, page_unit_xpaths)
    try:
        if page_unit:
            page_unit = page_unit.strip()[-1]
    except:
        print(f"í˜ì´ì§€ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨: {page_unit}")
    return page_unit

# ì—°ë ¹ ì¶”ì¶œ
def get_age(tree):
    age_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[5]'
    ]
    return extract_xpath(tree, age_xpaths)

def get_data_with_session(url_age_tuple, session=None):
    """ì„¸ì…˜ì„ ì‚¬ìš©í•œ ë‹¨ì¼ ì†Œì„¤ ë°ì´í„° ì¶”ì¶œ (19ê¸ˆìš©)"""
    url, age = url_age_tuple
    
    if session is None:
        session = create_session()
        should_close = True
    else:
        should_close = False
    
    headers = get_headers()
    
    max_retries = 3
    retry_count = 0
    
    try:
        while retry_count < max_retries:
            response = session.get(url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"HTTP {response.status_code} ì˜¤ë¥˜. ì¬ì‹œë„ {retry_count}/{max_retries}")
                    time.sleep(random.uniform(1, 3))
                    continue
                else:
                    raise Exception(f'HTTP {response.status_code} ì˜¤ë¥˜ê°€ {max_retries}ë²ˆ ë°˜ë³µë¨')
            break
        
        tree = html.fromstring(response.content)
        
        # ë°ì´í„° ì¶”ì¶œ
        img = get_img(tree)
        title = get_title(tree)
        rating = get_rating(tree)
        genre = get_genre(tree)
        serial = get_serial(tree)
        publisher, author = get_publisher_author(tree)
        summary = get_summary(tree)
        page_count = get_page_count(tree)
        page_unit = get_page_unit(tree)
        
        novel_data = {
            'url': url,
            'img': img,
            'title': title,
            'author': author,
            'rating': rating,
            'genre': genre,
            'serial': serial,
            'publisher': publisher,
            'summary': summary,
            'page_count': page_count,
            'page_unit': page_unit,
            'age': 19 if age == 19 else 'ì „ì²´',
            'platform': 'naver'
        }
        
        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit]):
        #     print(f"ë°ì´í„° ëˆ„ë½: {url}")
            # print(novel_data)
            raise Exception('ë°ì´í„° ëˆ„ë½ ë°œìƒ')
        
        # ë”œë ˆì´ ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(random.uniform(0.5, 1.5))
        return novel_data
        
    except Exception as e:
        # print(f"ë°ì´í„° ì¶”ì¶œ ì—ëŸ¬ {url}: {e}")
        return {}
    finally:
        if should_close:
            session.close()

def get_data(url_age_tuple):
    """ë‹¨ì¼ ì†Œì„¤ ë°ì´í„° ì¶”ì¶œ (ì „ì²´ ì´ìš©ê°€ìš©)"""
    return get_data_with_session(url_age_tuple, session=None)

def flatten(lst):
    """ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”"""
    result = []
    for item in lst:
        if isinstance(item, list):
            result.extend(item)
        else:
            result.append(item)
    return result

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def open_files(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

# ë°ì´í„° ì €ì¥í•˜ê¸°
def save_files(path, data):
    with open(path, 'wb') as f:
        pickle.dump(data, f)

def main():
    print("ğŸš€ ë„¤ì´ë²„ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘!")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    novel_data_path = 'data/naver_novel_data.data'
    novel_page_path = 'data/naver_page_links.link'
    
    # URL ìˆ˜ì§‘
    if os.path.exists(novel_page_path):
        all_urls = open_files(novel_page_path)
        # print(f"ê¸°ì¡´ URL ë°ì´í„° ë¡œë“œ: {len(all_urls)}ê°œ")
    else:
        print("URL ìˆ˜ì§‘ ì‹œì‘...")
        all_urls = []
        
        # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        max_page_num = get_last_page()
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜: {max_page_num}")
        
        # ëª¨ë“  í˜ì´ì§€ URL ìƒì„±
        page_urls = []
        for i in range(1, int(max_page_num.replace(',', '')) + 1):
            page_urls.append(f'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&page={i}')
        
        # parmapìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ ë§í¬ ìˆ˜ì§‘
        print("í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        all_links = parmap.map(get_links, page_urls, pm_pbar=True, pm_processes=4)
        
        # ê²°ê³¼ í‰íƒ„í™”
        all_urls = flatten(all_links)
        
        # ì €ì¥
        save_files(novel_page_path, all_urls)
    
    # all_urls = all_urls[:50]
    print(f"ì´ ìˆ˜ì§‘ëœ URL: {len(all_urls)}ê°œ")

    while True:
        try:
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
            if os.path.exists(novel_data_path):
                old_data = open_files(novel_data_path)
                cache_urls = [data['url'] for data in old_data if data]
                filtered_urls = [url for url in all_urls if url['url'] not in cache_urls]
                print(f"ìƒˆë¡œ í¬ë¡¤ë§í•  URL: {len(filtered_urls)}ê°œ")
            else:
                filtered_urls = all_urls
                old_data = []
            
            if not filtered_urls:
                print("í¬ë¡¤ë§í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ë‚˜ì´ë³„ë¡œ ë¶„ë¥˜
            not_nineteen_links = [link['url'] for link in filtered_urls if link['age'] == 0]
            nineteen_links = [link['url'] for link in filtered_urls if link['age'] == 19]
            
            all_results = []
            
            # not_nineteen_links = not_nineteen_links[:1000]
            print(f'ì „ì²´ ì´ìš©ê°€: {len(not_nineteen_links)}ê°œ')
            print(f'19ê¸ˆ: {len(nineteen_links)}ê°œ')
            
            # 1. ì „ì²´ ì´ìš©ê°€ ì†Œì„¤ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
            # if not_nineteen_links:
            #     print("ì „ì²´ ì´ìš©ê°€ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘...")
            #     not_nineteen_tuples = [(url, 0) for url in not_nineteen_links]
                
            #     # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬
            #     batches = split_data(not_nineteen_tuples, 20)  # 20ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
                
            #     for batch in tqdm(batches, desc="ì „ì²´ ì´ìš©ê°€ ë°°ì¹˜ ì²˜ë¦¬"):
            #         batch_results = parmap.map(
            #             get_data,
            #             batch,
            #             pm_pbar=False,  # ë°°ì¹˜ë³„ë¡œëŠ” ì§„í–‰ë¥  í‘œì‹œ ì•ˆí•¨
            #             pm_processes=5
            #         )
                    
            #         # None ê°’ ì œê±°í•˜ê³  ê²°ê³¼ ì¶”ê°€
            #         valid_batch_results = [result for result in batch_results if result is not None]
            #         all_results.extend(valid_batch_results)
                    
            #         # ë°°ì¹˜ ê°„ ë”œë ˆì´
            #         time.sleep(random.uniform(1, 4))
                
            #     print(f"ì „ì²´ ì´ìš©ê°€ í¬ë¡¤ë§ ì™„ë£Œ: {len([r for r in all_results if r['url'] == 'ì „ì²´'])}ê°œ")
            
            # 2. 19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ (Playwright ì‚¬ìš©)
            if nineteen_links:
                print("19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘...")
                print("Playwrightë¥¼ ì‚¬ìš©í•˜ì—¬ 19ê¸ˆ ì½˜í…ì¸ ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.")
                
                # ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ
                nineteen_results = asyncio.run(get_19(nineteen_links))
                all_results.extend(nineteen_results)
                
                print(f"19ê¸ˆ í¬ë¡¤ë§ ì™„ë£Œ: {len(nineteen_results)}ê°œ")
            
            print(f"ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_results)}ê°œ")
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
            all_data = old_data + all_results
            
            # ì €ì¥
            save_files(novel_data_path, all_data)
            print(f"ì´ {len(all_data)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            break
            
        except Exception as e:
            print(f'ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}')
            print('í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.')
            
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ì €ì¥
            if 'all_results' in locals() and all_results:
                if os.path.exists(novel_data_path):
                    existing_data = open_files(novel_data_path)
                    existing_data.extend(all_results)
                    save_files(novel_data_path, existing_data)
                else:
                    save_files(novel_data_path, all_results)
            break

if __name__ == "__main__":
    try:
        main()
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()