import requests
from lxml import html
import pandas as pd
import random
from itertools import chain
import os
from fake_useragent import UserAgent
from tqdm import tqdm
import pickle
import time
import parmap
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ì„¸ì…˜ ì„¤ì • (ì¬ì‚¬ìš© ë° ì¬ì‹œë„ ì„¤ì •)
def create_session():
    session = requests.Session()
    
    # ì¬ì‹œë„ ì„¤ì •
    retry_strategy = Retry(
        total=3,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["HEAD", "GET", "OPTIONS"],
        backoff_factor=1
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

# í—¤ë” ìƒì„±
def get_headers():
    ua = UserAgent(platforms='desktop')
    return {
        'User-Agent': ua.random,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

def login_and_get_cookies():
    """Seleniumì„ ì‚¬ìš©í•´ ë¡œê·¸ì¸ ë° ì„±ì¸ ì¸ì¦ í›„ ì¿ í‚¤ ë°˜í™˜"""
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        options = Options()
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
            driver.get('https://nid.naver.com/nidlogin.login')
            
            print("=" * 60)
            print("ğŸ” ë„¤ì´ë²„ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            print("=" * 60)
            print("ë¸Œë¼ìš°ì €ì—ì„œ ë„¤ì´ë²„ ë¡œê·¸ì¸ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”...")
            input()
            
            # ì„±ì¸ ì¸ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ 19ê¸ˆ ì†Œì„¤ í˜ì´ì§€ë¡œ ì´ë™
            test_url = 'https://series.naver.com/novel/detail.series?productNo=12340968'
            print(f"19ê¸ˆ ì½˜í…ì¸  ì ‘ê·¼ í…ŒìŠ¤íŠ¸: {test_url}")
            driver.get(test_url)
            time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
            
            # ì„±ì¸ ì¸ì¦ ìƒíƒœ í™•ì¸
            page_source = driver.page_source
            print("í˜ì´ì§€ ë¡œë”© ì™„ë£Œ, ì ‘ê·¼ ìƒíƒœ í™•ì¸ ì¤‘...")
            
            # ë‹¤ì–‘í•œ í‚¤ì›Œë“œë¡œ ì ‘ê·¼ ì œí•œ í™•ì¸
            restriction_keywords = [
                'ì„±ì¸ì¸ì¦', '19ì„¸ ì´ìƒ', 'ë³¸ì¸ì¸ì¦', 'ì„±ì¸ ì½˜í…ì¸ ',
                'adult_certification', 'ì„±ì¸ ì‘í’ˆ', 'ì—°ë ¹ ì œí•œ'
            ]
            
            is_restricted = any(keyword in page_source for keyword in restriction_keywords)
            
            if is_restricted:
                print("âŒ ì„±ì¸ ì½˜í…ì¸  ì ‘ê·¼ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
                print("ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
                print("1. ì„±ì¸ ì¸ì¦ì´ ì™„ë£Œë˜ì§€ ì•ŠìŒ")
                print("2. ê³„ì • ì„¤ì •ì—ì„œ ì„±ì¸ ì½˜í…ì¸  ë³´ê¸°ê°€ ë¹„í™œì„±í™”ë¨")
                print("3. ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ 19ê¸ˆ ì½˜í…ì¸ ì— ì ‘ê·¼í•˜ì—¬ ì¸ì¦ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”")
                print("\në¸Œë¼ìš°ì €ì—ì„œ ì„±ì¸ ì¸ì¦ì„ ì™„ë£Œí•œ í›„ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
                input()
                
                # ë‹¤ì‹œ í™•ì¸
                driver.refresh()
                time.sleep(3)
                page_source = driver.page_source
                
                if any(keyword in page_source for keyword in restriction_keywords):
                    print("âš ï¸ ì—¬ì „íˆ ì ‘ê·¼ì´ ì œí•œë©ë‹ˆë‹¤. ê·¸ë˜ë„ ì¿ í‚¤ë¥¼ ì €ì¥í•˜ì—¬ ì‹œë„í•´ë³´ê² ìŠµë‹ˆë‹¤.")
                else:
                    print("âœ… ì„±ì¸ ì½˜í…ì¸  ì ‘ê·¼ ì„±ê³µ!")
            else:
                print("âœ… ì„±ì¸ ì½˜í…ì¸  ì ‘ê·¼ ì„±ê³µ!")
            
            # ëª¨ë“  ë„ë©”ì¸ì—ì„œ ì¿ í‚¤ ìˆ˜ì§‘
            all_cookies = {}
            
            # í˜„ì¬ í˜ì´ì§€ ì¿ í‚¤
            for cookie in driver.get_cookies():
                all_cookies[cookie['name']] = cookie['value']
            
            # ë„¤ì´ë²„ ë©”ì¸ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì¶”ê°€ ì¿ í‚¤ ìˆ˜ì§‘
            driver.get('https://www.naver.com')
            time.sleep(2)
            for cookie in driver.get_cookies():
                all_cookies[cookie['name']] = cookie['value']
            
            # ë„¤ì´ë²„ ì‹œë¦¬ì¦ˆ ë©”ì¸ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì¶”ê°€ ì¿ í‚¤ ìˆ˜ì§‘
            driver.get('https://series.naver.com')
            time.sleep(2)
            for cookie in driver.get_cookies():
                all_cookies[cookie['name']] = cookie['value']
            
            print(f"ì¶”ì¶œëœ ì¿ í‚¤ ìˆ˜: {len(all_cookies)}ê°œ")
            
            # ì¤‘ìš” ì¿ í‚¤ë“¤ì´ ìˆëŠ”ì§€ í™•ì¸
            important_cookies = ['NID_AUT', 'NID_SES', 'NACT', 'nx_ssl']
            found_important = [name for name in important_cookies if name in all_cookies]
            print(f"ì¤‘ìš” ì¿ í‚¤ ë°œê²¬: {found_important}")
            
            return all_cookies
            
        finally:
            driver.quit()
            
    except ImportError:
        raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install selenium")
    except Exception as e:
        raise Exception(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {e}")

def get_data_with_selenium(url_age_tuple):
    """Seleniumì„ ì‚¬ìš©í•œ 19ê¸ˆ ë°ì´í„° ì¶”ì¶œ (ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê°œë³„ ë¸Œë¼ìš°ì € ì‹¤í–‰)"""
    url, age = url_age_tuple
    
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        # í—¤ë“œë¦¬ìŠ¤ Chrome ì„¤ì •
        options = Options()
        options.add_argument('--headless')  # ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-logging')
        options.add_argument('--disable-web-security')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option("detach", False)
        
        driver = webdriver.Chrome(options=options)
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        try:
            # ì¿ í‚¤ íŒŒì¼ì—ì„œ ë¡œê·¸ì¸ ì •ë³´ ë¡œë“œ
            cookies_path = 'data/naver_cookies.pickle'
            if os.path.exists(cookies_path):
                cookies = open_files(cookies_path)
                
                # ë„¤ì´ë²„ ë©”ì¸ìœ¼ë¡œ ì´ë™ í›„ ì¿ í‚¤ ì„¤ì •
                driver.get('https://www.naver.com')
                
                # ì¿ í‚¤ ì„¤ì •
                for name, value in cookies.items():
                    try:
                        driver.add_cookie({'name': name, 'value': value, 'domain': '.naver.com'})
                    except:
                        continue
                
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ìœ¼ë¡œ ì¿ í‚¤ ì ìš©
                driver.refresh()
                time.sleep(1)
            else:
                print(f"ì¿ í‚¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # 19ê¸ˆ í˜ì´ì§€ë¡œ ì´ë™
            driver.get(url)
            time.sleep(2)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            page_source = driver.page_source
            
            # ì ‘ê·¼ ì œí•œ í™•ì¸
            if any(keyword in page_source.lower() for keyword in 
                   ['ì„±ì¸ì¸ì¦', '19ì„¸ ì´ìƒ', 'ë³¸ì¸ì¸ì¦', 'ë¡œê·¸ì¸ì´ í•„ìš”']):
                print(f"19ê¸ˆ ì ‘ê·¼ ì œí•œ: {url}")
                return None
            
            # lxmlë¡œ íŒŒì‹±
            tree = html.fromstring(page_source)
            
            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ (ìš”ì•½ ì „ì²´ ë³´ê¸°)
            try:
                more_button = driver.find_element(By.XPATH, '//*[@id="content"]/div[2]/div[1]/span/a')
                driver.execute_script("arguments[0].click();", more_button)
                time.sleep(1)
                # ìƒˆë¡œìš´ í˜ì´ì§€ ì†ŒìŠ¤ë¡œ ë‹¤ì‹œ íŒŒì‹±
                tree = html.fromstring(driver.page_source)
            except:
                pass  # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
            
            # ë°ì´í„° ì¶”ì¶œ
            img = get_img(tree)
            title = get_title(tree)
            rating = get_rating(tree)
            genre = get_genre(tree)
            serial = get_serial(tree)
            publisher, author = get_publisher_author(tree)
            summary = get_summary(tree)
            page_count = get_page_count(tree)
            page_unit = get_page_unit(tree)
            
            novel_data = {
                'url': url,
                'img': img,
                'title': title,
                'author': author,
                'rating': rating,
                'genre': genre,
                'serial': serial,
                'publisher': publisher,
                'summary': summary,
                'page_count': page_count,
                'page_unit': page_unit,
                'age': 19,
                'platform': 'naver'
            }
            
            # í•„ìˆ˜ ë°ì´í„° ì²´í¬
            if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit]):
                print(f"ë°ì´í„° ëˆ„ë½: {url} - {title}")
                return None
            
            print(f"âœ… 19ê¸ˆ ì¶”ì¶œ ì„±ê³µ: {title}")
            time.sleep(random.uniform(1, 2))
            return novel_data
            
        finally:
            driver.quit()
            
    except ImportError:
        print("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    except Exception as e:
        print(f"Selenium ë°ì´í„° ì¶”ì¶œ ì—ëŸ¬ {url}: {e}")
        return None
        
        # ë°ì´í„° ì¶”ì¶œ
        img = get_img(tree)
        title = get_title(tree)
        rating = get_rating(tree)
        genre = get_genre(tree)
        serial = get_serial(tree)
        publisher, author = get_publisher_author(tree)
        summary = get_summary(tree)
        page_count = get_page_count(tree)
        page_unit = get_page_unit(tree)
        
        novel_data = {
            'url': url,
            'img': img,
            'title': title,
            'author': author,
            'rating': rating,
            'genre': genre,
            'serial': serial,
            'publisher': publisher,
            'summary': summary,
            'page_count': page_count,
            'page_unit': page_unit,
            'age': 19 if age == 19 else 'ì „ì²´',
            'platform': 'naver'
        }
        
        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit]):
            print(f"ë°ì´í„° ëˆ„ë½: {url}")
            # print(novel_data)
            raise Exception('ë°ì´í„° ëˆ„ë½ ë°œìƒ')
        
        # ë”œë ˆì´ ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(random.uniform(0.5, 1.5))
        return novel_data
        
    except Exception as e:
        print(f"ë°ì´í„° ì¶”ì¶œ ì—ëŸ¬ {url}: {e}")
        return None
    finally:
        session.close()

# ë°ì´í„° ë‚˜ëˆ„ê¸°
def split_data(data, split_num):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ë¶„í•  í•¨ìˆ˜"""
    if isinstance(data, list):
        new_data = []
        for i in range(0, len(data), split_num):
            new_data.append(data[i: i+split_num])
        return new_data
    elif isinstance(data, dict):
        items = list(data.items())
        new_data = []
        for i in range(0, len(items), split_num):
            batch_items = items[i: i+split_num]
            batch_dict = dict(batch_items)
            new_data.append(batch_dict)
        return new_data
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì…ë‹ˆë‹¤: {type(data)}")

# ë§í¬ ê°€ì ¸ì˜¤ê¸°
def get_links(url):
    session = create_session()
    headers = get_headers()
    
    try:
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        tree = html.fromstring(response.content)
        links = []
        
        # h3 íƒœê·¸ë“¤ ì°¾ê¸°
        h3_elements = tree.xpath('//*[@id="content"]/div/ul/li/div/h3')
        
        for h3_element in h3_elements:
            # ì œëª© ê°€ì ¸ì˜¤ê¸°
            page_title = h3_element.text_content().strip()
            
            # a íƒœê·¸ ì°¾ê¸° (h3 ë‚´ë¶€ì—ì„œ)
            a_elements = h3_element.xpath('.//a[contains(@href, "/novel/detail")]')
            
            if a_elements:
                href = a_elements[0].get('href')
                link = 'https://series.naver.com' + href
                
                # ë‚˜ì´ ì œí•œ ì²´í¬
                age = 19 if '19ê¸ˆ' in page_title else 0
                
                links.append({
                    'url': link,
                    'age': age,
                })
        
        time.sleep(random.randint(1, 2))
        return links
        
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì—ëŸ¬ {url}: {e}")
        return []
    finally:
        session.close()

def get_last_page():
    session = create_session()
    headers = get_headers()
    
    try:
        url = 'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&page=100000'
        response = session.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        tree = html.fromstring(response.content)
        page_elements = tree.xpath('//*[@id="content"]/p/a/text()')
        
        if page_elements:
            max_page_num = max(page_elements)
            return max_page_num
        return "1"
        
    except Exception as e:
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        return "1"
    finally:
        session.close()

# XPath ì¶”ì¶œ í•¨ìˆ˜
def extract_xpath(tree, xpaths, attr=None):
    for xpath in xpaths:
        try:
            elements = tree.xpath(xpath)
            if elements:
                if attr:
                    data = elements[0].get(attr)
                else:
                    data = elements[0].text_content()
                
                if data and data.strip():
                    if '_ë‹˜ë¡œê·¸ì•„ì›ƒ' in data:
                        continue
                    return data.strip()
        except Exception as e:
            continue
    return ''

# ì´ë¯¸ì§€ ì¶”ì¶œ
def get_img(tree):
    img_xpaths = [
        '/html/body/div[1]/div[2]/div[1]/span/img',
        '//*[@id="container"]/div[1]/a/img',
        '//*[@id="container"]/div[1]/span/img',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[1]/a/img'
    ]
    return extract_xpath(tree, img_xpaths, attr='src')

# ì œëª© ì¶”ì¶œ
def get_title(tree):
    title_xpaths = [
        '//*[@id="content"]/div[1]/h2',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/strong',
        '//*[@id="content"]/div[2]',
        '//*[@id="content"]/div[2]/h2'
    ]
    return extract_xpath(tree, title_xpaths)

# í‰ì  ì¶”ì¶œ
def get_rating(tree):
    rating_xpaths = [
        '//*[@id="content"]/div[1]/div[1]/em',
        '//*[@id="content"]/div[2]/div[1]',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[1]/ul/li/span/span',
    ]
    return extract_xpath(tree, rating_xpaths)

# ì¥ë¥´ ì¶”ì¶œ
def get_genre(tree):
    genre_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[2]/span/a',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[2]'
    ]
    return extract_xpath(tree, genre_xpaths)

# ì—°ì¬ ìƒíƒœ ì¶”ì¶œ
def get_serial(tree):
    serial_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[1]/span',
        '//*[@id="ct"]/div[1]/div[1]/div[1]/div[2]/div[2]/ul/li[1]/dl/dd[1]'
    ]
    return extract_xpath(tree, serial_xpaths)

# ì¶œíŒì‚¬ ë° ì‘ê°€ ì¶”ì¶œ
def get_publisher_author(tree):
    publisher_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[3]'
    ]
    author_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[4]/a',
        '//*[@id="content"]/ul[1]/li/ul/li[3]/a'
    ]
    publisher = extract_xpath(tree, publisher_xpaths)
    author = extract_xpath(tree, author_xpaths)
    return publisher, author

# ì„¤ëª… ì¶”ì¶œ
def get_summary(tree):
    summary_xpaths = [
        '//*[@id="content"]/div[2]/div[2]',
        '//*[@id="content"]/div[2]/div'
    ]
    return extract_xpath(tree, summary_xpaths)

# í™”ìˆ˜ ì¶”ì¶œ
def get_page_count(tree):
    page_count_xpaths = [
        '//*[@id="content"]/h5/strong'
    ]
    return extract_xpath(tree, page_count_xpaths)

# ë‹¨ìœ„ ì¶”ì¶œ
def get_page_unit(tree):
    page_unit_xpaths = [
        '//*[contains(@class, "end_total_episode")]'
    ]
    page_unit = extract_xpath(tree, page_unit_xpaths)
    try:
        if page_unit:
            page_unit = page_unit.strip()[-1]
    except:
        print(f"í˜ì´ì§€ ë‹¨ìœ„ ì¶”ì¶œ ì‹¤íŒ¨: {page_unit}")
    return page_unit

# ì—°ë ¹ ì¶”ì¶œ
def get_age(tree):
    age_xpaths = [
        '//*[@id="content"]/ul[1]/li/ul/li[5]'
    ]
    return extract_xpath(tree, age_xpaths)

def get_data_with_session(url_age_tuple, session=None):
    """ì„¸ì…˜ì„ ì‚¬ìš©í•œ ë‹¨ì¼ ì†Œì„¤ ë°ì´í„° ì¶”ì¶œ (ì „ì²´ ì´ìš©ê°€ìš©)"""
    url, age = url_age_tuple
    
    if session is None:
        session = create_session()
        should_close = True
    else:
        should_close = False
    
    headers = get_headers()
    
    max_retries = 3
    retry_count = 0
    
    try:
        while retry_count < max_retries:
            response = session.get(url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"HTTP {response.status_code} ì˜¤ë¥˜. ì¬ì‹œë„ {retry_count}/{max_retries}")
                    time.sleep(random.uniform(1, 3))
                    continue
                else:
                    raise Exception(f'HTTP {response.status_code} ì˜¤ë¥˜ê°€ {max_retries}ë²ˆ ë°˜ë³µë¨')
            break
        
        tree = html.fromstring(response.content)
        
        # ë°ì´í„° ì¶”ì¶œ
        img = get_img(tree)
        title = get_title(tree)
        rating = get_rating(tree)
        genre = get_genre(tree)
        serial = get_serial(tree)
        publisher, author = get_publisher_author(tree)
        summary = get_summary(tree)
        page_count = get_page_count(tree)
        page_unit = get_page_unit(tree)
        
        novel_data = {
            'url': url,
            'img': img,
            'title': title,
            'author': author,
            'rating': rating,
            'genre': genre,
            'serial': serial,
            'publisher': publisher,
            'summary': summary,
            'page_count': page_count,
            'page_unit': page_unit,
            'age': 19 if age == 19 else 'ì „ì²´',
            'platform': 'naver'
        }
        
        # í•„ìˆ˜ ë°ì´í„° ì²´í¬
        if not all([title, rating, genre, serial, publisher, summary, page_count, page_unit]):
            # print(f"ë°ì´í„° ëˆ„ë½: {url}")
            # print(novel_data)
            raise Exception('ë°ì´í„° ëˆ„ë½ ë°œìƒ')
        
        # ë”œë ˆì´ ì¶”ê°€ (ì„œë²„ ë¶€í•˜ ë°©ì§€)
        time.sleep(random.uniform(0.5, 1.5))
        return novel_data
        
    except Exception as e:
        # print(f"ë°ì´í„° ì¶”ì¶œ ì—ëŸ¬ {url}: {e}")
        return None
    finally:
        if should_close:
            session.close()

def get_data(url_age_tuple):
    """ë‹¨ì¼ ì†Œì„¤ ë°ì´í„° ì¶”ì¶œ (ì „ì²´ ì´ìš©ê°€ìš©)"""
    return get_data_with_session(url_age_tuple, session=None)

def flatten(lst):
    """ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™”"""
    result = []
    for item in lst:
        if isinstance(item, list):
            result.extend(item)
        else:
            result.append(item)
    return result

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def open_files(path):
    with open(path, 'rb') as f:
        return pickle.load(f)

# ë°ì´í„° ì €ì¥í•˜ê¸°
def save_files(path, data):
    with open(path, 'wb') as f:
        pickle.dump(data, f)

def main():
    print("ğŸš€ ë„¤ì´ë²„ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘!")
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    novel_data_path = 'data/naver_novel_data.data'
    novel_page_path = 'data/naver_page_links.link'
    cookies_path = 'data/naver_cookies.pickle'
    
    # URL ìˆ˜ì§‘
    if os.path.exists(novel_page_path):
        all_urls = open_files(novel_page_path)
        # print(f"ê¸°ì¡´ URL ë°ì´í„° ë¡œë“œ: {len(all_urls)}ê°œ")
    else:
        print("URL ìˆ˜ì§‘ ì‹œì‘...")
        all_urls = []
        
        # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        max_page_num = get_last_page()
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜: {max_page_num}")
        
        # ëª¨ë“  í˜ì´ì§€ URL ìƒì„±
        page_urls = []
        for i in range(1, int(max_page_num.replace(',', '')) + 1):
            page_urls.append(f'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&page={i}')
        
        # parmapìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ ë§í¬ ìˆ˜ì§‘
        print("í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
        all_links = parmap.map(get_links, page_urls, pm_pbar=True, pm_processes=4)
        
        # ê²°ê³¼ í‰íƒ„í™”
        all_urls = flatten(all_links)
        
        # ì €ì¥
        save_files(novel_page_path, all_urls)
    
    # all_urls = all_urls[:50]
    print(f"ì´ ìˆ˜ì§‘ëœ URL: {len(all_urls)}ê°œ")

    while True:
        try:
            # ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ìƒˆë¡œìš´ URLë§Œ í•„í„°ë§
            if os.path.exists(novel_data_path):
                old_data = open_files(novel_data_path)
                cache_urls = [data['url'] for data in old_data if data]
                filtered_urls = [url for url in all_urls if url['url'] not in cache_urls]
                print(f"ìƒˆë¡œ í¬ë¡¤ë§í•  URL: {len(filtered_urls)}ê°œ")
            else:
                filtered_urls = all_urls
                old_data = []
            
            if not filtered_urls:
                print("í¬ë¡¤ë§í•  ìƒˆë¡œìš´ URLì´ ì—†ìŠµë‹ˆë‹¤.")
                break
            
            # ë‚˜ì´ë³„ë¡œ ë¶„ë¥˜
            not_nineteen_links = [link['url'] for link in filtered_urls if link['age'] == 0]
            nineteen_links = [link['url'] for link in filtered_urls if link['age'] == 19]
            
            all_results = []
            
            # not_nineteen_links = not_nineteen_links[:1000]
            print(f'ì „ì²´ ì´ìš©ê°€: {len(not_nineteen_links)}ê°œ')
            print(f'19ê¸ˆ: {len(nineteen_links)}ê°œ')
            
            # 1. ì „ì²´ ì´ìš©ê°€ ì†Œì„¤ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
            # if not_nineteen_links:
            #     print("ì „ì²´ ì´ìš©ê°€ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘...")
            #     not_nineteen_tuples = [(url, 0) for url in not_nineteen_links]
                
            #     # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬
            #     batches = split_data(not_nineteen_tuples, 20)  # 20ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
                
            #     for batch in tqdm(batches, desc="ì „ì²´ ì´ìš©ê°€ ë°°ì¹˜ ì²˜ë¦¬"):
            #         batch_results = parmap.map(
            #             get_data,
            #             batch,
            #             pm_pbar=False,  # ë°°ì¹˜ë³„ë¡œëŠ” ì§„í–‰ë¥  í‘œì‹œ ì•ˆí•¨
            #             pm_processes=5
            #         )
                    
            #         # None ê°’ ì œê±°í•˜ê³  ê²°ê³¼ ì¶”ê°€
            #         valid_batch_results = [result for result in batch_results if result is not None]
            #         all_results.extend(valid_batch_results)
                    
            #         # ë°°ì¹˜ ê°„ ë”œë ˆì´
            #         time.sleep(random.uniform(1, 4))
                
            #     print(f"ì „ì²´ ì´ìš©ê°€ í¬ë¡¤ë§ ì™„ë£Œ: {len([r for r in all_results if r and r.get('age') == 'ì „ì²´'])}ê°œ")
            
            # 2. 19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ (Selenium ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬)
            if nineteen_links:
                print("19ê¸ˆ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘...")
                print("ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ ê°œë³„ Selenium ë¸Œë¼ìš°ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                
                # ë¨¼ì € ë¡œê·¸ì¸í•˜ì—¬ ì¿ í‚¤ ì €ì¥
                if not os.path.exists(cookies_path):
                    print("ë„¤ì´ë²„ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    try:
                        cookies = login_and_get_cookies()
                        save_files(cookies_path, cookies)
                        print("ë¡œê·¸ì¸ ì¿ í‚¤ ì €ì¥ ì™„ë£Œ")
                    except Exception as login_error:
                        print(f"ë¡œê·¸ì¸ ì‹¤íŒ¨: {login_error}")
                        print("19ê¸ˆ í¬ë¡¤ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                        break
                else:
                    print("ê¸°ì¡´ ì¿ í‚¤ íŒŒì¼ ì‚¬ìš©")
                
                # 19ê¸ˆ ë§í¬ íŠœí”Œ ìƒì„± (ì¿ í‚¤ ì •ë³´ ì—†ì´)
                nineteen_tuples = [(url, 19) for url in nineteen_links]
                
                # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ì„œ ì²˜ë¦¬
                batches = split_data(nineteen_tuples, 3)  # 3ê°œì”© ì²˜ë¦¬ (Seleniumì€ ë¦¬ì†ŒìŠ¤ ë§ì´ ì‚¬ìš©)
                
                for i, batch in enumerate(tqdm(batches, desc="19ê¸ˆ ë°°ì¹˜ ì²˜ë¦¬")):
                    print(f"\n19ê¸ˆ ë°°ì¹˜ {i+1}/{len(batches)} ì²˜ë¦¬ ì¤‘...")
                    batch_results = parmap.map(
                        get_data_with_selenium,  # Selenium í•¨ìˆ˜ ì‚¬ìš©
                        batch,
                        pm_pbar=False,
                        pm_processes=2  # Seleniumì€ 2ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ ì œí•œ
                    )
                    
                    # ê²°ê³¼ í™•ì¸ ë° í†µê³„
                    valid_batch_results = [result for result in batch_results if result is not None]
                    failed_count = len(batch) - len(valid_batch_results)
                    
                    print(f"ë°°ì¹˜ ê²°ê³¼: ì„±ê³µ {len(valid_batch_results)}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")
                    
                    all_results.extend(valid_batch_results)
                    
                    # ë°°ì¹˜ ê°„ ê¸´ ë”œë ˆì´ (ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹œê°„ í™•ë³´)
                    if i < len(batches) - 1:  # ë§ˆì§€ë§‰ ë°°ì¹˜ê°€ ì•„ë‹ˆë©´
                        delay = random.uniform(5, 8)
                        print(f"ë‹¤ìŒ ë°°ì¹˜ê¹Œì§€ {delay:.1f}ì´ˆ ëŒ€ê¸° (ë¸Œë¼ìš°ì € ì •ë¦¬ ì‹œê°„)...")
                        time.sleep(delay)
                
                print(f"19ê¸ˆ í¬ë¡¤ë§ ì™„ë£Œ: {len([r for r in all_results if r and r.get('age') == 19])}ê°œ")
            
            print(f"ì´ ìˆ˜ì§‘ëœ ë°ì´í„°: {len(all_results)}ê°œ")
            
            # ê¸°ì¡´ ë°ì´í„°ì™€ í•©ì¹˜ê¸°
            all_data = old_data + all_results
            
            # ì €ì¥
            save_files(novel_data_path, all_data)
            print(f"ì´ {len(all_data)}ê°œ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            break
            
        except Exception as e:
            print(f'ë°ì´í„° ì¶”ì¶œ ì˜¤ë¥˜: {e}')
            print('í˜„ì¬ê¹Œì§€ì˜ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.')
            
            # ë¶€ë¶„ ê²°ê³¼ë¼ë„ ì €ì¥
            if 'all_results' in locals() and all_results:
                if os.path.exists(novel_data_path):
                    existing_data = open_files(novel_data_path)
                    existing_data.extend(all_results)
                    save_files(novel_data_path, existing_data)
                else:
                    save_files(novel_data_path, all_results)
            break

if __name__ == "__main__":
    try:
        main()
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()