import asyncio
from playwright.async_api import async_playwright
import pandas as pd
import random
import os
from fake_useragent import UserAgent
from tqdm import tqdm
import pickle
from tqdm.asyncio import tqdm_asyncio
from fake_useragent import UserAgent

# ë°ì´í„° ë‚˜ëˆ„ê¸°
def split_data(data, split_num):
    """ë¦¬ìŠ¤íŠ¸ì™€ ë”•ì…”ë„ˆë¦¬ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” ë²”ìš© ë¶„í•  í•¨ìˆ˜"""
    
    if isinstance(data, list):
        # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
        new_data = []
        for i in range(0, len(data), split_num):
            new_data.append(data[i: i+split_num])
        return new_data
    
    elif isinstance(data, dict):
        # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš°
        items = list(data.items())  # (key, value) íŠœí”Œë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        new_data = []
        
        for i in range(0, len(items), split_num):
            batch_items = items[i: i+split_num]
            batch_dict = dict(batch_items)  # ë‹¤ì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            new_data.append(batch_dict)
        
        return new_data
    
    else:
        raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì…ì…ë‹ˆë‹¤: {type(data)}")

# ìµœëŒ€ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°
async def get_last_page(page, url):
    """ê°€ì¥ ê¸°ë³¸ì ì¸ ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§ - í˜ì´ì§€ ì¬ì‚¬ìš©"""
    try:
        # í˜ì´ì§€ ë°©ë¬¸
        await page.goto(url)
        
        await page.wait_for_selector('.col-next')
        # í´ë¦­ ëŒ€ê¸° ë° ì‹¤í–‰
        await page.locator('.col-next').click()

        # í˜ì´ì§€ ë²ˆí˜¸ ìš”ì†Œë“¤ ê°€ì ¸ì˜¤ê¸°
        await page.wait_for_selector('xpath=//*[@id="NOVELOUS-CONTENTS"]/section[4]/ul')
        page_num = await page.locator('.col-xs-2').all()
        temp = []
        for n in page_num:
            text = await n.inner_text()
            if text.isdigit():
                temp.append(text)
            # temp.append(0)

        temp = [int(i) for i in temp if i]
        max_num = max(temp)
        return max_num
        
    except Exception as e:
        print(f"ìµœëŒ€ í˜ì´ì§€ ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        print(page.content())
        return 1

# ë§í¬ ê°€ì ¸ì˜¤ê¸°
async def get_links(page, url):
    """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë§í¬ ìˆ˜ì§‘"""
    try:
        await page.goto(url)
        # await page.wait_for_load_state('networkidle')

        links = []
        elements = await page.locator('xpath=/html/body/div[8]/div[3]/div[6]/div/table/tbody/tr[1]/td[1]').all()
        
        for element in elements:
            onclick_value = await element.get_attribute('onclick')
            # print(onclick_value)
            if onclick_value:
                # onclick="location='/novel/25974';" ì—ì„œ URL ë¶€ë¶„ ì¶”ì¶œ
                if "location='" in onclick_value:
                    url_part = onclick_value.split("location='")[1].split("'")[0]
                    full_url = f"https://novelpia.com{url_part}"
                    links.append(full_url)
                    
                    # ì œëª©ë„ í•¨ê»˜ ì¶”ì¶œ
                    title = await element.inner_text()
                    # print(f"ì œëª©: {title}, URL: {full_url}")
        
        await asyncio.sleep(random.uniform(4, 15))
        return links
        
    except Exception as e:
        print(f"ë§í¬ ìˆ˜ì§‘ ì—ëŸ¬ {url}: {e}")
        return []

async def save_data(results):
    """ê²°ê³¼ ì €ì¥ í•¨ìˆ˜"""
    if not results:
        print("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    # ì „ì²´ ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(results)
    filename = f'data/novelpia_novel_data.csv'
    df.to_csv(filename, encoding='utf-8', index=False)
    
    print(f'ğŸ“ ì „ì²´ ë°ì´í„° ì €ì¥: {filename} ({len(results)}ê°œ)')

async def extract_element(page, xpaths, type='text'):        
    # ì¶”ì²œ ìˆ˜
    for xpath in xpaths:
        if await page.locator(xpath).count() > 0:
            if type == 'img':
                img = await page.locator(xpath).get_attribute('src')
                return img
            data = await page.locator(xpath).inner_text()
            return data


async def main():
    print("ğŸš€ ë…¸ë²¨í”¼ì•„ ì†Œì„¤ í¬ë¡¤ë§ ì‹œì‘!")
    
    # User Agent ì„¤ì •
    ua = UserAgent()
    user_agent = ua.random
    # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs('data', exist_ok=True)
    
    async with async_playwright() as playwright:
        # ë‹¨ì¼ ë¸Œë¼ìš°ì € ë° í˜ì´ì§€ ìƒì„±
        browser = await playwright.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor',
                '--no-first-run',
                '--disable-extensions',
                '--disable-default-apps',
                '--disable-gpu'
            ]
        )
        
        context = await browser.new_context(
            user_agent=user_agent,
            is_mobile=False,
            has_touch=False,
            viewport={'width': 1920, 'height': 1080}
        )
        
        page = await context.new_page()

        try:

            # ë§í¬ ë°ì´í„° í™•ì¸ ë˜ëŠ” ìˆ˜ì§‘
            if os.path.exists('data/munpia_novel_page_link_data.data'):
                print("ğŸ“‚ ê¸°ì¡´ ë§í¬ ë°ì´í„° ë¡œë“œ ì¤‘...")
                with open('data/munpia_novel_page_link_data.data', 'rb') as f:
                    all_links = pickle.load(f)
            else:
                print("ğŸ” ìƒˆë¡œìš´ ë§í¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
                
                # ìµœëŒ€ í˜ì´ì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                last_page_num = await get_last_page(page, 'https://novel.munpia.com/page/hd.platinum/group/pl.serial/view/serial')
                print(f"ì´ í˜ì´ì§€ ìˆ˜: {last_page_num}")
                
                # # URL ìƒì„± (í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì²˜ìŒ 2í˜ì´ì§€ë§Œ)
                # urls = [f"https://novelpia.com/plus/all/date/{i}/?main_genre=&is_please_write=" for i in range(1, last_page_num)]  # í…ŒìŠ¤íŠ¸ìš©
                
                # # ê° í˜ì´ì§€ì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ ë§í¬ ìˆ˜ì§‘
                # all_links = []
                # for url in tqdm(urls, desc="ğŸ“„ í˜ì´ì§€ë³„ ë§í¬ ìˆ˜ì§‘"):
                #     links = await get_links(page, url)
                #     all_links.extend(links)
                #     print(f"ìˆ˜ì§‘ëœ ë§í¬ ìˆ˜: {len(links)}")
                
                # # ë§í¬ ë°ì´í„° ì €ì¥
                # with open('data/novelpia_novel_page_link_data.data', 'wb') as f:
                #     pickle.dump(all_links, f)
                # print(f"ğŸ“ ì´ {len(all_links)}ê°œ ë§í¬ ì €ì¥ ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ë©”ì¸ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
        
        finally:
            await browser.close()

            # ë°ì´í„° ì €ì¥
            # if all_results:
            #     print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì¤‘... ({len(all_results)}ê°œ)")
            #     await save_data(all_results)
            # else:
            #     print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
        print("\nğŸŠ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì¤‘ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()